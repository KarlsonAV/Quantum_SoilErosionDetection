{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nTVDJ0ASzt6x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from patchify import unpatchify\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UNET model architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrReH932zt6z",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, smooth=1e-6, gama=2):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.name = 'NDL'\n",
    "        self.smooth = smooth\n",
    "        self.gama = gama\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true, y_pred = tf.cast(\n",
    "            y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
    "        nominator = 2 * \\\n",
    "            tf.reduce_sum(tf.multiply(y_pred, y_true)) + self.smooth\n",
    "        denominator = tf.reduce_sum(\n",
    "            y_pred ** self.gama) + tf.reduce_sum(y_true ** self.gama) + self.smooth\n",
    "        result = 1 - tf.divide(nominator, denominator)\n",
    "        return result\n",
    "\n",
    "\n",
    "def f1_metric(y_pred, y_true):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def IoU(y_pred, y_true):\n",
    "    I = tf.reduce_sum(y_pred * y_true, axis=(1, 2))\n",
    "    U = tf.reduce_sum(y_pred + y_true, axis=(1, 2)) - I\n",
    "    return tf.reduce_mean(I / U)\n",
    "\n",
    "def unet(pretrained_weights=None, input_size=(512, 512, 3)):\n",
    "\t\tinputs = Input(input_size)\n",
    "\t\tconv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "\t\tconv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "\t\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\t\tconv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "\t\tconv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "\t\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\t\tconv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "\t\tconv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "\t\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\t\tconv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "\t\tconv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "\t\tdrop4 = Dropout(0.5)(conv4)\n",
    "\t\tpool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "\t\tconv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "\t\tconv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\t\tdrop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "\t\tup6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "\t\tmerge6 = concatenate([drop4, up6], axis=3)\n",
    "\t\tconv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "\t\tconv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "\t\tup7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "\t\tmerge7 = concatenate([conv3, up7], axis=3)\n",
    "\t\tconv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "\t\tconv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "\t\tup8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "\t\tmerge8 = concatenate([conv2, up8], axis=3)\n",
    "\t\tconv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "\t\tconv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "\t\tup9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "\t\tmerge9 = concatenate([conv1, up9], axis=3)\n",
    "\t\tconv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "\t\tconv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\t\tconv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\t\tconv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "\t\tmodel = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "\t\tiou = tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)\n",
    "\n",
    "\t\tmodel.compile(optimizer=Adam(lr=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\t\tif pretrained_weights:\n",
    "\t\t\t\tmodel.load_weights(pretrained_weights)\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting images and masks from folder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t7zIZkStzt61"
   },
   "outputs": [],
   "source": [
    "raster_ims = []\n",
    "mask_ims = []\n",
    "\n",
    "N_AUGMENTS = 1\n",
    "for i in range(N_AUGMENTS):\n",
    "    rasters_path = f\"Data/patches/augmentation_{i}/images\"\n",
    "    masks_path = f\"Data/patches/augmentation_{i}/masks\"\n",
    "\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            raster_im = cv2.imread(f\"{rasters_path}/patch_raster_{j}_{k}.png\")\n",
    "            raster_im = cv2.cvtColor(raster_im, cv2.COLOR_RGB2BGR)\n",
    "            raster_ims.append(raster_im)\n",
    "\n",
    "            mask_im = cv2.imread(f\"{masks_path}/patch_mask_{j}_{k}.png\", cv2.IMREAD_GRAYSCALE)\n",
    "            mask_ims.append(mask_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6ckkxp_1zt63"
   },
   "outputs": [],
   "source": [
    "raster_ims = np.array(raster_ims)\n",
    "mask_ims = np.array(mask_ims)\n",
    "mask_ims = np.expand_dims(mask_ims, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qajS9pwRzt65",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "raster_ims = raster_ims / 255.0\n",
    "mask_ims = mask_ims / 255.0"
   ],
   "metadata": {
    "id": "2n1jjSJUw2vw"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = unet()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33VwMUNy2TKG",
    "outputId": "fdb1e267-873e-4d45-c14f-afc96cbdd1d1"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_path = \"unet.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             verbose=1)"
   ],
   "metadata": {
    "id": "-VScE8imJ2_X"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history = net.fit(raster_ims, mask_ims, epochs=10, batch_size=3, callbacks=[checkpoint])"
   ],
   "metadata": {
    "id": "A2TBLz_Q2W6f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "944fd3c2-bdcf-48b3-902b-09bb9d9fc11b"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.9905"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 141s 785ms/step - loss: 0.6642 - accuracy: 0.9905\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5932 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 118s 801ms/step - loss: 0.5932 - accuracy: 0.9973\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 807ms/step - loss: 0.5346 - accuracy: 0.9973\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 809ms/step - loss: 0.4825 - accuracy: 0.9973\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 810ms/step - loss: 0.4362 - accuracy: 0.9973\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3950 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 810ms/step - loss: 0.3950 - accuracy: 0.9973\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 809ms/step - loss: 0.3584 - accuracy: 0.9973\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 809ms/step - loss: 0.3259 - accuracy: 0.9973\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 809ms/step - loss: 0.2969 - accuracy: 0.9973\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9973"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/147 [==============================] - 119s 810ms/step - loss: 0.2711 - accuracy: 0.9973\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "predicted = []\n",
    "\n",
    "for img in raster_ims:\n",
    "    predicted.append(net.predict(np.expand_dims(img, axis=0)))"
   ],
   "metadata": {
    "id": "YXUQjX3VyCjN"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predicted1 = np.array(predicted).reshape((441, 512, 512, 1))\n",
    "\n",
    "predicted1[predicted1 >= 0.3] = 1\n",
    "predicted1[predicted1 < 0.3] = 0\n",
    "predicted1 = predicted1 * 255"
   ],
   "metadata": {
    "id": "FX7KvH2OSbD_"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Recreating tile image\n",
    "result = unpatchify(predicted1, (10752, 10752, 1))\n",
    "result = np.squeeze(result)"
   ],
   "metadata": {
    "id": "QazZaXFpVWPM"
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "kp26d9fjXRAd",
    "outputId": "70c71431-2bde-493d-b163-740ddc561493"
   },
   "execution_count": 63,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f697d527650>"
      ]
     },
     "metadata": {},
     "execution_count": 63
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling Results"
   ],
   "metadata": {
    "id": "Y6iH8-0sYGwK",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately my computer doesn't allow me to take advantage of GPU training boosting and due to that, I had to rebuilt entire modeling process in google colabs. And because of that, I couldn't upload to google collabs all the augmentations that were created, therefore, a big amount of my work was lost (Used only 414 images out of +-5000). Our model did very badly, because of all computational-resources obstacles which have occurred during the modeling process. In order to improve results it would be reasonable to use a Jaccard Index as a loss function. Accuracy is a bad metric for imbalanced data (the one we have) and I'd replace it with IoU score (useful for image segmentation) and F1-score (useful for unbalanced dataset). The main problem here is a very poor amount of data (only 1 tile), which has very little instances of soil erosion."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6e8bd8f2d50af7579fedacaa7885e08a3ef54616cecbaea48fe079c940a8496"
   }
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}